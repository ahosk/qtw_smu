{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 1: Predicting Critical Temperature\n",
    "By: Allen Hoskins\n",
    "\n",
    "***\n",
    "# Introduction\n",
    "\n",
    "Superconductors are materials that conduct electricity with little or no resistance without onset or buildup of heat. Due to this process, superconductors can create a magnetic field and generate a constant flow of electricity. These materials have a property called `critical temperature`. This property is the temperature at which this material acts as a superconductor. While most materials have an extremely low critical temperature (between 0 and 10 Kelvin), research has been ongoing to find materials with higher critical temperatures.\n",
    "\n",
    "In this case study, we will utilize Linear Regression with both L1 and L2 regularization to predict the critical temperature of a compound to potentially identify superconductors.\n",
    "***\n",
    "\n",
    "# METHODS\n",
    "\n",
    "### DATA PREPROCESSING:\n",
    "\n",
    "The original data is composed of two separate CSV files, named `train.csv` and `unique_m.csv`. Once importing the needed packages for the case study, I read in the data and determined the shape and size of both datasets. The `train.csv` dataset consisted of 21,263 rows with 82 columns, and the `unique_m.csv` dataset consisted of 21,263 rows with 88 columns. The two datasets were then able to be merged on the index, and the duplicate response variable of `critical_temp` was able to be dropped, resulting in a final dataframe consisting of 21,263 rows with 168 columns. Before proceeding to Exploratory Data Analysis (EDA), I checked variable datatypes to determine if any other preprocessing needed to be done. With every datatype consisting of a float or integer, I was able to move on to EDA.\n",
    "\n",
    "### EXPLORATORY DATA ANALYSIS (EDA):\n",
    "\n",
    "With little information about the 168 explanatory variables and response variable within the data set, I needed to determine if the data needed to be scaled. I first plotted a histogram of the response variable to see the distribution (Fig. 1). The histogram showed that the `critical_temp` was heavily right skewed. To determine if any other variables in the data were heavily skewed, I ran quick descriptive statistics and plotted histograms of the following explanatory variables: `number_of_elements`, `entropy_atomic_mass`, `wtd_mean_atomic_mass`, `critical_temp`. All of these variables appeared to have a large variances and non-standard distributions (Fig 2). \n",
    "\n",
    "<figure>\n",
    "  <figcaption>Fig. 1</figcaption>\n",
    "  <img\n",
    "  src=\"./images/crit_temp_hist.png\">\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "  <figcaption>Fig. 2</figcaption>\n",
    "  <img\n",
    "  src=\"./images/variable_hist.png\">\n",
    "</figure>\n",
    "\n",
    "### LINEAR REGRESSION ASSUMPTIONS:\n",
    "\n",
    "Before performing a linear regssion model, I needed to check the assumptions. These consisted of:\n",
    "\n",
    "> Linearity: PASS\n",
    "\n",
    "> Homoscedasticity: FAIL\n",
    "\n",
    "> Independence: PASS\n",
    "\n",
    "> Normality: FAIL\n",
    "\n",
    "After testing the assumptions and determinign that they did not all pass, I needed to prepare the data with transformations below.\n",
    "\n",
    "### PREP DATA TO MODEL:\n",
    "\n",
    "Before scaling any of the explanatory variables, the data was separated into explanatory variables (X) and response variable (y). Once separated, I was able to scale the data. Sklearn provides multiple scalers to transform the data, but only `StandardScaler` and `PowerTransformer` were considered at this time. According to Sklearn's website, the definitions of the scalers are as follows:\n",
    "\n",
    "> StandardScaler: Standardize features by removing the mean and scaling to unit variance.\n",
    "> PowerTransformer: Apply a power transform feature wise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Supports Yeo-Johnson and Box-Cox.\n",
    "\n",
    "Both StandardScaler and PowerTransformer were run though the ElasticNet model, and since PowerTransformer produced the lowest mean RMSE, this will be the data set used moving forward. Note that since there are positive and negative integers in the data set, Yeo-Johnson was used instead of Box-Cox. Fig. 3 contains histograms of variables in Fig. 2 after utilizing `PowerTransformer`.\n",
    "\n",
    "<figure>\n",
    "  <figcaption>Fig. 3</figcaption>\n",
    "  <img\n",
    "  src=\"./images/variable_hist_post_powertransformer.png\">\n",
    "</figure>\n",
    "\n",
    "***\n",
    "# RESULTS\n",
    "\n",
    "For modeling, I determined that using Sklearn's `ElasticNetCV` model was appropriate as it combines the `l1` and `l2` regularization of `Lasso` and `Ridge` models.\n",
    "\n",
    "*Models use: 10-fold Cross validation (`Kfold`), `random_seed = 0`, and `max_iter = 20000`.*\n",
    "\n",
    "**Model GridSearchCV:**\n",
    "\n",
    "After preprocessing, EDA, and scaling the data, modeling was able to begin. Utilizing the power of Sklearn's `GridSearchCV`, the hyperparameters of `l1_ratio`, `tol`, and `eps` were run though the model and the best output using the `neg_root_mean_squared_error` scoring were output to be used in the final model.\n",
    "\n",
    "**Grid Search Parameters:**\n",
    "\n",
    "```\n",
    "      \"l1_ratio\": np.arange(0.0, 1.0, 0.1), \n",
    "      \"tol\":      [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n",
    "      \"eps\":      [1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
    "```\n",
    "\n",
    "**Best Model Output:**\n",
    "\n",
    "```\n",
    "      \"l1_ratio\": 0.2\n",
    "      \"tol\":      0.01\n",
    "      \"eps\":      0.001\n",
    "```\n",
    "\n",
    "**ElasticNetCV with GridSearchCV Tuned Parameters:**\n",
    "\n",
    "After performing GridSearchCV to tune the model parameters, Sklearn's `cross_validate` was used to validate the model and determine final performance. The results of all 10 folds are below with a mean RMSE of 16.4637.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>fit_time</td>\n",
    "        <td>score_time</td>\n",
    "        <td>estimator</td>\n",
    "        <td>test_score</td>\n",
    "        <td>train_score</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>2.19916</td>\n",
    "        <td>0.00117993</td>\n",
    "        <td>ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01)</td>\n",
    "        <td>16.7378</td>\n",
    "        <td>16.3605</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>2.15888</td>\n",
    "        <td>0.00172806</td>\n",
    "        <td>ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01)</td>\n",
    "        <td>16.7811</td>\n",
    "        <td>16.3389</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td>2.09171</td>\n",
    "        <td>0.00159836</td>\n",
    "        <td>ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01)</td>\n",
    "        <td>16.2488</td>\n",
    "        <td>16.4508</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>3</td>\n",
    "        <td>2.13909</td>\n",
    "        <td>0.00134301</td>\n",
    "        <td>ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01)</td>\n",
    "        <td>16.4203</td>\n",
    "        <td>16.4222</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>4</td>\n",
    "        <td>2.04526</td>\n",
    "        <td>0.00174427</td>\n",
    "        <td>ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01)</td>\n",
    "        <td>16.5608</td>\n",
    "        <td>16.4041</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>5</td>\n",
    "        <td>2.05884</td>\n",
    "        <td>0.00163698</td>\n",
    "        <td>ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01)</td>\n",
    "        <td>15.7617</td>\n",
    "        <td>16.4887</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>6</td>\n",
    "        <td>2.1159</td>\n",
    "        <td>0.00138688</td>\n",
    "        <td>ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01)</td>\n",
    "        <td>16.2624</td>\n",
    "        <td>16.446</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>7</td>\n",
    "        <td>2.06758</td>\n",
    "        <td>0.00157595</td>\n",
    "        <td>ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01)</td>\n",
    "        <td>16.7527</td>\n",
    "        <td>16.397</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>8</td>\n",
    "        <td>2.10658</td>\n",
    "        <td>0.00192094</td>\n",
    "        <td>ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01)</td>\n",
    "        <td>16.4909</td>\n",
    "        <td>16.4127</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>9</td>\n",
    "        <td>2.11754</td>\n",
    "        <td>0.00175214</td>\n",
    "        <td>ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01)</td>\n",
    "        <td>16.6208</td>\n",
    "        <td>16.4005</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>MEAN</td>\n",
    "        <td>2.11005</td>\n",
    "        <td>0.00158665</td>\n",
    "        <td></td>\n",
    "        <td>16.4637</td>\n",
    "        <td>16.4121</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**Feature Importance:**\n",
    "\n",
    "After completing the ElasticNetCV modeling, I wanted to determine which features were the most important in predicting `critical_temp`.\n",
    "\n",
    "The top 10 features in the model were:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>Feature</td>\n",
    "        <td>Coefficient</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>19</td>\n",
    "        <td>Cu</td>\n",
    "        <td>4.99197</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>7</td>\n",
    "        <td>Ba</td>\n",
    "        <td>4.88638</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>12</td>\n",
    "        <td>Ca</td>\n",
    "        <td>4.83552</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>38</td>\n",
    "        <td>La</td>\n",
    "        <td>-3.48937</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>163</td>\n",
    "        <td>wtd_std_Valence</td>\n",
    "        <td>-3.24568</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>9</td>\n",
    "        <td>Bi</td>\n",
    "        <td>2.40671</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>162</td>\n",
    "        <td>wtd_std_ThermalConductivity</td>\n",
    "        <td>2.29054</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>57</td>\n",
    "        <td>Pr</td>\n",
    "        <td>-2.27065</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>31</td>\n",
    "        <td>Hg</td>\n",
    "        <td>2.2417</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>146</td>\n",
    "        <td>wtd_mean_ThermalConductivity</td>\n",
    "        <td>1.89972</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "***\n",
    "\n",
    "# CONCLUSION\n",
    "\n",
    "In conclusion, the best model that was chosen used a combination of L1 and L2 regularization (`l1_ratio = 0.2`). This model produced a mean RMSE of 16.4637. While not all models are useful, the output of this model can be used as a base in determining superconductors.\n",
    "***\n",
    "\n",
    "# CODE:\n",
    "\n",
    "Attached in file CS1_CODE.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "64a98b05f8fc9eecf2bb5338f68e06ef5a87c8de5d8181b9d94599cc0446591d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
