{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 1: Predicting Critical Temperature\n",
    "By: Allen Hoskins\n",
    "\n",
    "***\n",
    "# 1. Introduction\n",
    "\n",
    "Superconductiors are materials that conduct electricity wiht little or no resistance without onset or buildup of heat. Due to this process,superconductors can create a magnetic field and generate constant flow of electricity. These matericals have a property called `critical temperature`. This properity is the temperature at which this materical acts as a superconductor. While most matericals have an extremly low critical temperatures (between 0 and 10 Kelvin), research has been ongoing to find matericals with higher critical temperatures.\n",
    "\n",
    "In this case study, we will utilize Linear Regression with both L1 and L2 regularization to predict the critical temperature of a compound to potentially identify superconductors.\n",
    "***\n",
    "\n",
    "# 2. METHODS\n",
    "\n",
    "### DATA PREPROCESSING\n",
    "\n",
    "The origianl data is composed of two separate CSV files, named `train.csv` and `unique_m.csv`. Once importing needed packages for the case study, I read in the data and determined shape and size of both datasets. The `train.csv` dataset consisted of 21,263 rows with 82 columns and the `unique_m.csv` dataset consisted of 21,263 rows with 88 columns. The two datasets were then able to be merged on the index and the duplicate response variable of `critical_temp` was able to be dropped resuling in a final dataframe consisting of 21,263 rows with 168 columns. Before proceeding to Exploratory Data Analyis (EDA), I checked variable datatypes to determine if any other preprocessing needed to be done. With every datatype consisting of a float or integer, I was able to move to EDA.\n",
    "\n",
    "\n",
    "### EXPLORATORY DATA ANALYSIS (EDA)\n",
    "\n",
    "With little information about the 168 explanitory variables and response variable within the data set, I needed to determine if the data needed to be scaled. I first plotted a histogram of the response variable to see the distribution (fig. 1). The histogram showed that the `critical_temp` was heavily right skewed. To determine if any of the data was heavily skewed, I ran quick descriptive statitics and plotted historgrams of the following explanitory variables: `number_of_elements`, `entropy_atomic_mass`, `wtd_mean_atomic_mass`, `critical_temp`. All of which appeared to have a large variance and non-standard distributions (fig 2). \n",
    "\n",
    "<figure>\n",
    "  <figcaption>Fig. 1</figcaption>\n",
    "  <img\n",
    "  src=\"./superconduct/crit_temp_hist.png\">\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "  <figcaption>Fig. 2</figcaption>\n",
    "  <img\n",
    "  src=\"./superconduct/variable_hist.png\">\n",
    "</figure>\n",
    "\n",
    "\n",
    "### PREP DATA TO MODEL\n",
    "\n",
    "Before scaling any of the explanatory variables, the data we separated out into explanatory variables (X) and response (y). Once separated, I was able to scale the data. Sklearn provides multiple scalers to transform the data, but only `StandardScaler` and `PowerTransformer` were considered a this time. According to Sklearn's website the definition of the scalers are such:\n",
    "\n",
    "> StandardScaler: Standardize features by removing the mean and scaling to unit variance.\n",
    "> PowerTransformer: Apply a power transform featurewise to make data more Gaussian-like. Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired. Supports Yeo-Johnson and Box-Cox.\n",
    "\n",
    "Both StandardScaler and PowerTransformer were run though the ElasticNet model and since PowerTransformer produced the lowest mean RMSE, this will be the data set used moving forward. Note that since there are positive and negative integers in the data set, Yeo-Johnson was used instead of Box-Cox.\n",
    "\n",
    "***\n",
    "# 3. RESULTS\n",
    "\n",
    "For modeling, I determind that using Sklearn's `ElasticNetCV` model was appropriate as it combines the `l1` and `l2` regularization of `Lasso` and `Ridge` models.\n",
    "\n",
    "*Models use: 10-fold Cross validation (`Kfold`), `random_seed = 0`, and `max_iter = 20000`.*\n",
    "\n",
    "**Model GridSearchCV:**\n",
    "\n",
    "After preprocessing, EDA, and scaling the data, modeling was able to begin. Utilizing the power of Sklearn's `GridSearchCV`, the hyperparameters of `l1_ratio`, `tol`, and `eps` were run though the model and the best output using the `neg_root_mean_squared_error` scoring were output to be used in the final model.\n",
    "\n",
    "**Grid Search Parameters:**\n",
    "\n",
    "```\n",
    "      \"l1_ratio\": np.arange(0.0,1.0,0.1), \n",
    "      \"tol\":      [1e-9,1e-8,1e-7,1e-6,1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n",
    "      \"eps\":      [1e-3, 1e-2, 1e-1,1,10,100]\n",
    "```\n",
    "\n",
    "**Best Model Output:**\n",
    "\n",
    "```\n",
    "      \"l1_ratio\": 0.2\n",
    "      \"tol\":      0.01\n",
    "      \"eps\":      0.001\n",
    "```\n",
    "\n",
    "**ElasticNetCV with GridSearchCV Tuned Parameters:**\n",
    "\n",
    "After performing GridSearchCV to tune the model parameters, Sklearn's `cross_validate` was used to validate the model and determine final performance. The results of all 10 folds are below with a mean RMSE of 16.4637.\n",
    "\n",
    "|      |   fit_time |   score_time | estimator                                                            |  test_score |  train_score |\n",
    "|:----:|-----------:|-------------:|:---------------------------------------------------------------------|------------:|-------------:|\n",
    "| 0    |    2.19916 |   0.00117993 | ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01) |     16.7378 |      16.3605 |\n",
    "| 1    |    2.15888 |   0.00172806 | ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01) |     16.7811 |      16.3389 |\n",
    "| 2    |    2.09171 |   0.00159836 | ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01) |     16.2488 |      16.4508 |\n",
    "| 3    |    2.13909 |   0.00134301 | ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01) |     16.4203 |      16.4222 |\n",
    "| 4    |    2.04526 |   0.00174427 | ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01) |     16.5608 |      16.4041 |\n",
    "| 5    |    2.05884 |   0.00163698 | ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01) |     15.7617 |      16.4887 |\n",
    "| 6    |    2.1159  |   0.00138688 | ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01) |     16.2624 |      16.446  |\n",
    "| 7    |    2.06758 |   0.00157595 | ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01) |     16.7527 |      16.397  |\n",
    "| 8    |    2.10658 |   0.00192094 | ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01) |     16.4909 |      16.4127 |\n",
    "| 9    |    2.11754 |   0.00175214 | ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01) |     16.6208 |      16.4005 |\n",
    "| mean |    2.11005 |   0.00158665 |                                                                      |     16.4637 |      16.4121 |\n",
    "\n",
    "**Feature Importance:**\n",
    "\n",
    "After completeing the ElasticNetCV modeling, I wanted to determine which features were the most important in predicting `critical_temp`.\n",
    "\n",
    "The top 10 features in the model were:\n",
    "\n",
    "|     | Feature                      |   Coefficient |\n",
    "|:---:|:-----------------------------|--------------:|\n",
    "|  19 | Cu                           |       4.99197 |\n",
    "|   7 | Ba                           |       4.88638 |\n",
    "|  12 | Ca                           |       4.83552 |\n",
    "|  38 | La                           |      -3.48937 |\n",
    "| 163 | wtd_std_Valence              |      -3.24568 |\n",
    "|   9 | Bi                           |       2.40671 |\n",
    "| 162 | wtd_std_ThermalConductivity  |       2.29054 |\n",
    "|  57 | Pr                           |      -2.27065 |\n",
    "|  31 | Hg                           |       2.2417  |\n",
    "| 146 | wtd_mean_ThermalConductivity |       1.89972 |\n",
    "***\n",
    "\n",
    "# 4. CONCLUSION\n",
    "\n",
    "In conclusion, the best model that was chosen used a combination of L1 and L2 regularization (`l1_ratio = 0.2`). This model produced an mean RMSE of 16.4637. While not all models are useful, the output of this model can be used as a base in determining superconductors.\n",
    "***\n",
    "\n",
    "# Start Code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import \n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import  GridSearchCV,KFold, cross_validate, cross_val_predict\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cufflinks as cf\n",
    "import plotly as py\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "cf.go_offline()\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "ConvergenceWarning('ignore')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r'./superconduct/train.csv')\n",
    "df_unique_m = pd.read_csv(r'./superconduct/unique_m.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge two dataframes on indexes\n",
    "df_merge = pd.merge(df_train, df_unique_m, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete duplicate and unused column\n",
    "df_merge = df_merge.drop(['critical_temp_y','material'], axis=1)\n",
    "\n",
    "#rename column from merge\n",
    "df_merge.rename(columns = {'critical_temp_x':'critical_temp'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain basic information about data types for dataframes\n",
    "\n",
    "df_train_info = df_train.info(verbose=True)\n",
    "df_unique_info = df_unique_m.info(verbose = True)\n",
    "df_merge_info = df_merge.info(verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check shape of dataframes\n",
    "df_train_shape = df_train.shape\n",
    "df_unique_m_shape = df_unique_m.shape\n",
    "df_merge_shape = df_merge.shape\n",
    "print(f'Train DataFrame Shape: {df_train_shape}')\n",
    "print(f'Unique_m DataFrame Shape:{df_unique_m_shape}')\n",
    "print(f'Merged DataFrame Shape:{df_merge_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain basic descriptive statistics\n",
    "df_merge.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit_temp_hist = df_merge['critical_temp'].iplot(\n",
    "                                                kind='hist',\n",
    "                                                bins=100,\n",
    "                                                xTitle='Critical Temperature',\n",
    "                                                linecolor='black',\n",
    "                                                yTitle='count',\n",
    "                                                title='Histogram of Critical Temperature')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_cols = ['number_of_elements','entropy_atomic_mass','wtd_mean_atomic_mass','wtd_range_atomic_mass']\n",
    "df_scatter = df_merge[scatter_cols]\n",
    "df_scatter.iplot(kind='histogram',opacity=.75,title='Variable Histograms',subplots=True,xTitle='Value',yTitle='Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Begin Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset of columns to transform\n",
    "scale_cols = df_merge.columns[df_merge.columns != 'critical_temp']\n",
    "\n",
    "# Scale Columns\n",
    "#sc = StandardScaler()\n",
    "sc = PowerTransformer()\n",
    "df_merge[scale_cols] = sc.fit_transform(df_merge[scale_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Features after Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_cols = ['number_of_elements','mean_atomic_mass','mean_atomic_radius','critical_temp']\n",
    "df_scatter_scaled = df_merge[scatter_cols]\n",
    "scaled_scatter_matrix = df_scatter_scaled.scatter_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specififying Stratified Kfold for cv.\n",
    "kfcv = KFold(n_splits=10,random_state=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set target and feature columns\n",
    "target_col = ['critical_temp']\n",
    "feature_cols = df_merge.loc[:, ~df_merge.columns.isin(target_col)].columns\n",
    "y = df_merge.critical_temp\n",
    "X = df_merge[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Grid search for Linear Regression task 1\n",
    "\n",
    "lr_grid={\"l1_ratio\":np.arange(0.0,1.0,0.1), \n",
    "      \"tol\": [1e-9,1e-8,1e-7,1e-6,1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n",
    "      \"eps\":[1e-3, 1e-2, 1e-1,1,10,100]\n",
    "      }\n",
    "\n",
    "model=ElasticNetCV(random_state = 0,max_iter=20000)\n",
    "\n",
    "model_gs=GridSearchCV(model,\n",
    "                       lr_grid,\n",
    "                       cv = kfcv,\n",
    "                       n_jobs=-1,\n",
    "                       scoring = \"neg_root_mean_squared_error\")\n",
    "\n",
    "model_gs.fit(X,y)\n",
    "best_params = model_gs.best_params_\n",
    "print(f'Grid Search Best Parameters{best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = ElasticNetCV(l1_ratio =.2 ,\n",
    "                        tol =0.01,\n",
    "                        eps =0.001,\n",
    "                        random_state = 0,\n",
    "                        max_iter = 10000)\n",
    "\n",
    "model_score = cross_validate(model_, X, y,\n",
    "                            scoring='neg_root_mean_squared_error',\n",
    "                            cv=kfcv,\n",
    "                            return_estimator=True,\n",
    "                            n_jobs=-1,\n",
    "                            return_train_score=True)\n",
    "\n",
    "model_results = pd.DataFrame(model_score)\n",
    "model_results.loc['mean'] = model_results.mean()\n",
    "print(model_results.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_pred = cross_val_predict(model_, X, y, cv=kfcv ,n_jobs=-1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y, cv_pred, edgecolors=(0, 0, 0))\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], \"k--\", lw=4)\n",
    "ax.set_xlabel(\"Actual\")\n",
    "ax.set_ylabel(\"Predicted\")\n",
    "ax.set_title(\"Elastic Net CV: Predicted vs Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain average coefficent for each feature\n",
    "df = pd.DataFrame()\n",
    "for i in range(10):\n",
    "    df_ = pd.DataFrame(list(zip(model_score['estimator'][i].coef_, X.columns)),columns = ['Coefficient','Feature'])\n",
    "    df = pd.concat([df_,df],axis=0)\n",
    "\n",
    "avg_feat_coef = df.groupby('Feature', as_index=False)['Coefficient'].mean()\n",
    "avg_feat_coef = avg_feat_coef.sort_values(by='Coefficient', key=abs, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_features = avg_feat_coef.head(10)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.barh(top_10_features['Feature'],top_10_features['Coefficient'])\n",
    "plt.title('Top 10 Features in Elastic Net CV Model')\n",
    "plt.ylabel('Feature')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.yticks(rotation=30, va='top')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('QTW')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64a98b05f8fc9eecf2bb5338f68e06ef5a87c8de5d8181b9d94599cc0446591d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
