{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 2: Predicting Critical Temperature\n",
    "By: Allen Hoskins and Brittany Lewandowski\n",
    "\n",
    "September 19, 2022 \n",
    "***\n",
    "# 1. INTRODUCTION \n",
    " \n",
    "Diabetes is a metabolic disease impacting 37.3 million Americans. Those affected by the disease have complications producing insulin, a chemical messenger that our body uses to store energy. Although it is uncommon, diabetics can be hospitalized for having critically low or high blood glucose levels. These hospitalizations can be life threatening and should be minimized at all costs.  \n",
    " \n",
    "In this case study, we will use a diabetes data set procured by Dr. Slater, to identify what factors most significantly result in diabetics getting readmitted to hospitals. To accomplish this, we will build a Logistic Regression model and extract its respective feature importances. It is our hope that this research can be leveraged by medical professionals to help treat hospitalized diabetics and to ensure that these patients are not readmitted in the future.  \n",
    " \n",
    "***\n",
    "# 2. METHODS \n",
    " \n",
    "#### DATA UNDERSTANDING: \n",
    " \n",
    "Data used in this case study was a diabetes.csv provided by Dr. Slater. Our diabetes.csv contained data related to hospitalized diabetic patients including columns such as: “readmitted,” “patient_nbr,” “insulin,” and “time_in_hospital.” Upon reviewing the contents of our data set, we saved the data into a data frame named “diabetes_data” and began pre-processing. \n",
    " \n",
    "#### DATA PREPROCESSING: \n",
    " \n",
    "The first step we performed in pre-processing was reviewing our full data set. Immediately, we recognized that missing values existed in the columns of:  \n",
    " \n",
    "1.\trace \n",
    "2.\tweight \n",
    "3.\tpayer_code \n",
    "4.\tmedical_specialty \n",
    "5.\tdiag_1 \n",
    "6.\tdiag_2 \n",
    "7.\tdiag_3 \n",
    "\n",
    "Given that machine learning models do not handle missing values well, we imputed them using appropriate statistical methods. Full details on how these columns were imputed can be found in the sub-header of this case study titled, “Data Imputation.”\n",
    "After identifying that missing values existed in our data set, we ran the command, “diabetes_data.info()” and noted the following details of our data frame:\n",
    "\n",
    ">\tOur data frame contained 101,766 rows.\n",
    ">\tOur data frame contained 50 columns. \n",
    ">\tNo null values existed in our data frame. \n",
    ">\tOur data frame contained 13 numeric columns. \n",
    ">\tOur data frame contained 37 categorical columns. \n",
    "\n",
    "From this output we recognized that one hot encoding (OHE), would need to be performed on our categorical columns. For additional details on our OHE process, please see the sub-header of this case study titled, “One Hot Encoding.”\n",
    "\n",
    "The next step performed in pre-processing was running the command “diabetes_data.describe() to view the summary statistics of our data frame. Output from this command showed that several columns contained outliers. This was something we remained cognizant of throughout our analysis. \n",
    "Finally, to view the distributions of our categorical columns with missing values, we created count plots. Visualizing these columns was important, as it helped us determine what data imputation method was most appropriate for our data. Output from our count plots showed that all seven of our columns with missing data contained non-normal distributions (Exhibit 1.0). Since all seven columns were of the categorical data type, we noted that imputing these columns with either the mean or median value would be appropriate. \n",
    "\n",
    "<br><center><figure>\n",
    "    <img\n",
    "        src=\"../images/count_plot_payer_code.png\" class=\"center\">\n",
    "    <figcaption>\n",
    "        <font size=\"+1\"><em><center>\n",
    "            Exhibit 1.0: Count Plot of Payer_Code\n",
    "        </center></em></font>\n",
    "    </figcaption><br>\n",
    "</figure></center>\n",
    "\n",
    "\n",
    "The last step performed in pre-processing was calculating the percentage of missing values in our categorical columns. Exhibit 1.1 details our findings:\n",
    "\n",
    "<center><table border=\"1\">\n",
    "<thead>\n",
    "<tr><th>Column Name</th><th>Percentage of Missing Values</th></tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td>race</td><td>2.23%</td></tr>\n",
    "<tr><td>weight</td><td>96.85%</td></tr>\n",
    "<tr><td>payer_code</td><td>39.55%</td></tr>\n",
    "<tr><td>medical_specialty</td><td>49.08%</td></tr>\n",
    "<tr><td>diag_1</td><td>0.02%</td></tr>\n",
    "<tr><td>diag_2</td><td>0.35%</td></tr>\n",
    "<tr><td>diag_3</td><td>1.39%</td></tr>\n",
    "</tbody>\n",
    "</table><br>\n",
    "<caption><font size=\"+1\"><em>Exhibit 1.1: Percent Missing Values</em></caption></center>\n",
    "\n",
    "\n",
    "#### DATA IMPUTATION: \n",
    "\n",
    "Upon reviewing our full data set and calculating the percentage of missing values that existed in our categorical columns, we proceeded to impute our missing values. \n",
    "\n",
    "All the missing values in our data set were denoted by: “?”. Since computers cannot impute data with special characters, we converted the question marks to “NaN”. Once this was complete, we re-calculated the sum of missing values in our columns to validate that no data loss had occurred in our conversion process.  \n",
    "\n",
    "When we considered imputing the columns: “race”, “payer_code”, “medical_specialty,” “diag_1,” “diag_2” and “diag_3”, we tried two different approaches. One approach was imputing these columns with the mode of each column, and the second approach was leaving the columns as is with missing values. We fit our Logistic Regression model on both approaches and found that our performance results were negligible. Consequently, we decided to leave the columns with missing values, as we felt this represented our data the best.  \n",
    "\n",
    "The first column we chose to impute was our “weight” column. Given that 96% of the data in our “weight” column were missing, we chose to drop the column from our data set. \n",
    "\n",
    "Next, we imputed the columns: “diabetesMed,” “change,” and “readmitted” with values of 0 and 1. This was done to simplify OHE as these columns had a maximum of three classes. Please note that although the column: “readmitted” contains three classes, we chose to convert it to a binary variable as we are only concerned with whether a patient has been readmitted or not. Exhibit 1.2 details our conversion process of these columns:\n",
    "\n",
    "<center><table>\n",
    "  <tr>\n",
    "    <th>Column Name</th>\n",
    "    <th>Original Classes</th>\n",
    "    <th>Data Dictionary for Converted Classes</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>diabetesMed</td>\n",
    "    <td>No<br>Yes</td>\n",
    "    <td>0=No<br>1=Yes</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Change</td>\n",
    "    <td>Ch<br>No</td>\n",
    "    <td>0=No<br>1=Yes</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>readmitted</td>\n",
    "    <td>NO<br>&#60;30<br>&#62;30</td>\n",
    "    <td>0=No<br>1=Yes</td>\n",
    "  </tr>\n",
    "</table><br>\n",
    "<caption><font size=\"+1\"><em>Exhibit 1.2: Imputation process for the columns: “diabetesMed”, “change\", and” readmitted”</em></caption></center>\n",
    "\n",
    "\n",
    "#### RE-CODING CATEGORICAL COLUMNS:\n",
    "\n",
    "When viewing the shape of our data set, we recognized that if we one hot encoded all 37 of our categorical variables, that our data set would be extremely wide. As a result, we decided to reduce the classes in each categorical variable by specifying a threshold for infrequent observations. Exhibit 1.3 details the thresholds that were chosen for each variable, as well as explanations as to why thresholds were chosen.\n",
    "\n",
    "<center><table border=\"1\">\n",
    "<thead>\n",
    "<tr><th>Column Name</th><th>Selected Threshold</th><th>Explanation</th></tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td>payer_code</td><td>0.02</td><td>~90% of our data falling into the top 7 classes</td></tr>\n",
    "<tr><td>medical_specialty</td><td>0.03</td><td>~85% of our data falling into the top 5 classes</td></tr>\n",
    "<tr><td>max_glu_serum</td><td>0.02</td><td>~96% of our data falling into the top 2 classes</td></tr>\n",
    "<tr><td>A1Cresult</td><td>0.08</td><td>~91% of our data falling into the top 2 classes</td></tr>\n",
    "<tr><td>metformin</td><td>0.1</td><td>~98% of our data falling into the top 2 classes</td></tr>\n",
    "<tr><td>repalglinide</td><td>0.01</td><td>~99% of our data falling into the top 2 classes</td></tr>\n",
    "<tr><td>nateglinide</td><td>0.9</td><td>~99% of our data falling into the top class</td></tr>\n",
    "<tr><td>chloropropamide</td><td>0.9</td><td>~99% of our data falling into the top class</td></tr>\n",
    "<tr><td>glimepiride</td><td>0.9</td><td>~95% of our data falling into the top class</td></tr>\n",
    "<tr><td>glipizide</td><td>0.1</td><td>~97 of our data falling into the top 2 classes</td></tr>\n",
    "<tr><td>glyburide</td><td>0.09</td><td>~98 of our data falling into the top 2 classes</td></tr>\n",
    "<tr><td>pioglitazone</td><td>0.06</td><td>~98 of our data falling into the top 2 classes</td></tr>\n",
    "<tr><td>rosiglitazone</td><td>0.05</td><td>~98% of our data falling into the top 2 classes</td></tr>\n",
    "<tr><td>acarbose</td><td>0.9</td><td>~99% of our data falling into the top class</td></tr>\n",
    "<tr><td>miglitol</td><td>0.9</td><td>~99% of our data falling into the top class</td></tr>\n",
    "<tr><td>tolazamide</td><td>0.0004</td><td>~99% of our data falls into the top class</td></tr>\n",
    "<tr><td>glyburide_metformin</td><td>0.9</td><td>~99% of our data falling into the class “No”</td></tr>\n",
    "<tr><td>diag_1</td><td>0.0075</td><td>many of the columns contained values <= 0.000010</td></tr>\n",
    "<tr><td>diag_2</td><td>0.0075</td><td>many of the columns contained values <= 0.000010</td></tr>\n",
    "<tr><td>diag_3</td><td>0.0075</td><td>many of the columns contained values <= 0.000010</td></tr>\n",
    "</tbody>\n",
    "</table><br>\n",
    "<caption><font size=\"+1\"><em>Exhibit 1.3: Detailed Recoding Threshold</em></caption></center>\n",
    "\n",
    "#### ONE HOT ENCODING: \n",
    "\n",
    "Once we imputed our missing values and re-coded our categorical columns we separated our diabetes data set into two variables, one containing all our numeric columns and the other containing all our categorical columns. Next, using Pandas’ get dummies function, we one hot encoded our categorical columns and joined our one hot encoded data to our numeric columns to arrive at our final full data set. \n",
    "\n",
    "Please note that for modeling, we scaled our non-hot encoded data to ensure that our full data set was on the same scale. For additional modeling details, please see our sub-header below titled “Modeling.”\n",
    "\n",
    "\n",
    "#### EXPLORATORY DATA ANALYSIS: \n",
    "\n",
    "For our exploratory data analysis (EDA), we began by viewing histograms and pair plots of our data (Exhibits 1.4 & 1.5). Two takeaways from these visualizations included:\n",
    "\n",
    "1.\tMany of our numeric columns exhibited non-normal distributions. \n",
    "2.\tOur variables were not on the same scale.\n",
    "\n",
    "<center><figure>\n",
    "    <img\n",
    "        src=\"../images/encounter_id_density.png\" class=\"center\">\n",
    "    <figcaption>\n",
    "      <font size=\"+1\"><em>\n",
    "            <em>Exhibit 1.4: Encounter ID Density Plot</em></font>\n",
    "    </figcaption><br>\n",
    "</figure></center>\n",
    "\n",
    "\n",
    "<center><figure>\n",
    "    <img\n",
    "        src=\"../images/admission_type_id_density.png\" class=\"center\">\n",
    "    <figcaption>\n",
    "        <font size=\"+1\"><em>\n",
    "            Exhibit 1.5: Admission Type ID Density Plot</em></font>\n",
    "    </figcaption><br>\n",
    "</figure></center>\n",
    "\n",
    "\n",
    "After reviewing our histograms and pair plots, we assessed multicollinearity in our data by creating a correlation plot. Seeing that no columns had a correlation coefficient of 1.0, we chose not to remove any columns from our data set. At this point we were satisfied with our data and began evaluating if our data met our Logistic Regression modeling assumptions.  \n",
    "\n",
    "#### ASSUMPTIONS OF LOGISTIC REGRESSION MODELS: \n",
    " \n",
    "The three key assumptions of Logistic Regression models include:\n",
    "1.\tIndependent variables have a linear relationship to the log loss of the response. \n",
    "2.\tAbsence of multicollinearity. \n",
    "3.\tLack of outliers. \n",
    "\n",
    "To assess our first assumption, we created two log odds linear plots of our response variable versus the independent variables (Exhibit 1.6): “time_in_hospital” and “num_medications” \n",
    "\n",
    "<center><figure>\n",
    "    <img\n",
    "        src=\"../images/time_in_hosp_log_odds.png\" class=\"center\">\n",
    "    <figcaption>\n",
    "      <font size=\"+1\"><em>\n",
    "            <em>Exhibit 1.6: Log Odds for Time in Hospital</em></font>\n",
    "    </figcaption><br>\n",
    "</figure></center>\n",
    "\n",
    "\n",
    "<center><figure>\n",
    "    <img\n",
    "        src=\"../images/num_med_log_odds.png\" class=\"center\">\n",
    "    <figcaption>\n",
    "        <font size=\"+1\"><em>\n",
    "            Exhibit 1.7: Log Odds for Num Meds Feature</em></font>\n",
    "    </figcaption><br>\n",
    "</figure></center>\n",
    "\n",
    "<center><figure>\n",
    "    <img\n",
    "        src=\"../images/orig_pair_plot.png\" class=\"center\"> \n",
    "    <figcaption>\n",
    "        <font size=\"+1\"><em>\n",
    "            Exhibit 1.8: Snapshot of several pair plots generated from our data</em></font>\n",
    "    </figcaption><br>\n",
    "</figure></center>\n",
    "\n",
    "\n",
    "Seeing that our log odds plots showed that our independent variables had a linear relationship to the log loss our response, we deemed that our first assumption was met. \n",
    "\n",
    "To address our multicollinearity assumption, we created a correlation plot. Given that no columns had a correlation coefficient of 1.0, we proceeded assuming that this assumption was met. \n",
    "\n",
    "The final assumption we addressed was lack of outliers. (Exhibit 1.8). As illustrated in our pair plots below, we did see that our data contained outliers. Given that the pair plots were built on un-scaled data, we proceeded in our analysis assuming that this assumption was met. \n",
    "\n",
    "<center><figure>\n",
    "    <img\n",
    "        src=\"../images/trans_log_reg_pair_plot.png\" class=\"center\">\n",
    "    <figcaption>\n",
    "        <font size=\"+1\"><em>\n",
    "            Exhibit 1.9: Addressing our Logistic Regression outlier assumption with pair plots</em></font>\n",
    "    </figcaption><br>\n",
    "</figure></center>\n",
    "\n",
    "\n",
    "***\n",
    "# 3. MODEL BUILDING & RESULTS\n",
    "\n",
    "The use of Sklearn's `LogisticRegression` was used to model the data for this case study.\n",
    "\n",
    "*Models use: 10-fold Cross validation (`Kfold`), `random_seed = 0`, and `max_iter = 50000`, and scoring metric of `F1`*\n",
    "\n",
    "**Model HalvingRandomSearchCV:**\n",
    "\n",
    "\n",
    "After preprocessing, EDA, and scaling the data, modeling was able to begin. To determine the best hyperparameters that we should use, we needed to iterate through several of `sklearn's` modules. We began with utilizing `GridSearchCV`, but due to the shape of our data and inability to scale our CPU, GPU, and Memory for the needs of this project, `GridSearchCV` was unable to complete and we needed to try other methods of tuning hyperparameters. With the use of Skelearn's `experimental` and `model_selection` packages we were able to utilize `HalvingRandomSearchCV` to obtain good, but potentially not the best hyperparameters for this model. `HalvingRandomSearchCV` combines the idea of `HalvingSearchCV` and `RandomizedSearchCV`.\n",
    "HalvingSearchCV works by modeliong all potential candidates with less data and selects half of the best performing models to add additional resources and data until a \"best\" model is output. `RandomizedSearchCV` randomly picks candidate modles from the grid to model. \n",
    "\n",
    "We passed the below parameters into `HalvingRandomSearchCV` and the best model outputs were the following:\n",
    "\n",
    "**Halving Random Search CV Parameters:**\n",
    "\n",
    "```\n",
    "        \"C\":            np.logspace(-3,3,7), \n",
    "        \"l1_ratio\":     np.arange(0.0,1.0,0.1),\n",
    "        'solver':       ['saga'],\n",
    "        'penalty':      ['elasticnet'],\n",
    "        \"tol\":          [1e-9,1e-8,1e-7,1e-6,1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "```\n",
    "\n",
    "**Best Model Output:**\n",
    "\n",
    "```\n",
    "        \"C\":            100.0\n",
    "        \"l1_ratio\":     0.8\n",
    "        \"n_jobs\":       -1\n",
    "        \"penalty\":      'elasticnet'\n",
    "        \"solver\":       'saga'\n",
    "        \"tol\":          0.001\n",
    "```\n",
    "\n",
    "**ElasticNetCV with GridSearchCV Tuned Parameters:**\n",
    "\n",
    "After performing `HalvingRandomSearchCV` to tune the model parameters, Sklearn's `cross_validate` was used to validate the model and determine final performance. The results of all 10 folds are below with a mean F1 score of .568746.\n",
    "\n",
    "<center><table>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th>fit_time</th>\n",
    "        <th>score_time</th>\n",
    "        <th>estimator</th>\n",
    "        <th>test_score</th>\n",
    "        <th>train_score</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>2.43238</td>\n",
    "        <td>0.00473499</td>\n",
    "        <td>LogisticRegression(C=100.0, l1_ratio=0.8, n_jobs=-1, penalty='elasticnet',random_state=0, solver='saga', tol=0.001)</td>\n",
    "        <td>0.568733</td>\n",
    "        <td>0.568562</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>2.60263</td>\n",
    "        <td>0.00420213</td>\n",
    "        <td>LogisticRegression(C=100.0, l1_ratio=0.8, n_jobs=-1, penalty='elasticnet',random_state=0, solver='saga', tol=0.001)</td>\n",
    "        <td>0.56834</td>\n",
    "        <td>0.568627</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td>2.9399</td>\n",
    "        <td>0.00471711</td>\n",
    "        <td>LogisticRegression(C=100.0, l1_ratio=0.8, n_jobs=-1, penalty='elasticnet',random_state=0, solver='saga', tol=0.001)</td>\n",
    "        <td>0.567456</td>\n",
    "        <td>0.568824</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>3</td>\n",
    "        <td>2.86625</td>\n",
    "        <td>0.00472498</td>\n",
    "        <td>LogisticRegression(C=100.0, l1_ratio=0.8, n_jobs=-1, penalty='elasticnet',random_state=0, solver='saga', tol=0.001)</td>\n",
    "        <td>0.566277</td>\n",
    "        <td>0.56902</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>4</td>\n",
    "        <td>2.24447</td>\n",
    "        <td>0.00615811</td>\n",
    "        <td>LogisticRegression(C=100.0, l1_ratio=0.8, n_jobs=-1, penalty='elasticnet',random_state=0, solver='saga', tol=0.001)</td>\n",
    "        <td>0.56726</td>\n",
    "        <td>0.568922</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>5</td>\n",
    "        <td>2.248</td>\n",
    "        <td>0.00481105</td>\n",
    "        <td>LogisticRegression(C=100.0, l1_ratio=0.8, n_jobs=-1, penalty='elasticnet',random_state=0, solver='saga', tol=0.001)</td>\n",
    "        <td>0.572271</td>\n",
    "        <td>0.568409</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>6</td>\n",
    "        <td>2.93157</td>\n",
    "        <td>0.00560784</td>\n",
    "        <td>LogisticRegression(C=100.0, l1_ratio=0.8, n_jobs=-1, penalty='elasticnet',random_state=0, solver='saga', tol=0.001)</td>\n",
    "        <td>0.574194</td>\n",
    "        <td>0.567999</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>7</td>\n",
    "        <td>2.92202</td>\n",
    "        <td>0.00460625</td>\n",
    "        <td>LogisticRegression(C=100.0, l1_ratio=0.8, n_jobs=-1, penalty='elasticnet',random_state=0, solver='saga', tol=0.001)</td>\n",
    "        <td>0.573015</td>\n",
    "        <td>0.568162</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>8</td>\n",
    "        <td>2.47317</td>\n",
    "        <td>0.00449204</td>\n",
    "        <td>LogisticRegression(C=100.0, l1_ratio=0.8, n_jobs=-1, penalty='elasticnet',random_state=0, solver='saga', tol=0.001)</td>\n",
    "        <td>0.561812</td>\n",
    "        <td>0.56956</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>9</td>\n",
    "        <td>2.8959</td>\n",
    "        <td>0.00579691</td>\n",
    "        <td>LogisticRegression(C=100.0, l1_ratio=0.8, n_jobs=-1, penalty='elasticnet',random_state=0, solver='saga', tol=0.001)</td>\n",
    "        <td>0.568101</td>\n",
    "        <td>0.568796</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>MEAN</td>\n",
    "        <td>2.65563</td>\n",
    "        <td>0.00498514</td>\n",
    "        <td></td>\n",
    "        <td>0.568746</td>\n",
    "        <td>0.568688</td>\n",
    "    </tr>\n",
    "</table></center>\n",
    "\n",
    "***\n",
    "\n",
    "# 4. CONCLUSION\n",
    "\n",
    "In conclusion, after significant updates to thresholds and hyperparameters, we have determined that logistic regression does not properly model this data due to inablilty for the coefficient's to converge. Potential models that woudl be better for this data set would include decision trees or any sort of gradient boosting. \n",
    "\n",
    "***\n",
    "\n",
    "# 5. CODE:\n",
    "\n",
    "Attached in file CS2_CODE.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "64a98b05f8fc9eecf2bb5338f68e06ef5a87c8de5d8181b9d94599cc0446591d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
