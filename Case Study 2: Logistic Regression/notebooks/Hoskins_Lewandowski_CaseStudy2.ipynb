{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 2: Predicting Critical Temperature\n",
    "By: Allen Hoskins and Brittany Lewandowski\n",
    "\n",
    "***\n",
    "# 1. INTRODUCTION\n",
    "\n",
    "\n",
    "Diabetes is a metabolic disease impacting 37.3 million Americans. Those affected by the disease have complications producing insulin, a chemical messenger that our body uses to store energy. Although it is uncommon, diabetics can be hospitalized for having critically low or high blood glucose levels. These hospitalizations can be life threatening and should be minimized at all costs.  \n",
    "\n",
    " \n",
    "\n",
    "In this case study, we will use a diabetes data set procured by Dr. Slater, to identify what factors most significantly result in diabetics getting readmitted to hospitals. To accomplish this, we will build a Logistic Regression model and extract its respective feature importances. It is our hope that this research can be leveraged by medical professionals to help treat hospitalized diabetics and to ensure that these patients are not readmitted in the future.  \n",
    "\n",
    "***\n",
    "\n",
    "# 2. METHODS\n",
    "\n",
    "\n",
    "#### DATA UNDERSTANDING: \n",
    "\n",
    "Data used in this case study was a diabetes.csv provided by Dr. Slater. Our diabetes.csv contained data related to hospitalized diabetic patients including columns such as: “readmitted,” “patient_nbr,” “insulin,” and “time_in_hospital.” Upon reviewing the contents of our data set, we saved the data into a dataframe named “diabetes_data” and began pre-processing. \n",
    "\n",
    "\n",
    "#### DATA PREPROCESSING: \n",
    "\n",
    "\n",
    "The first step we performed in pre-processing was reviewing our full data set. Immediately, we recognized that missing values existed in the columns of:  \n",
    "\n",
    "> race \n",
    "> weight \n",
    "> payer_code \n",
    "> medical_specialty \n",
    "> diag_1 \n",
    "> diag_2 \n",
    "> diag_3 \n",
    "\n",
    "After identifying these missing values, we then ran the command, `diabetes_data.dtypes.value_counts()` and saw that our data set contained 37 categorical columns and 13 numeric columns. Noticing that our data set contained categorical columns, we noted that one hot encoding would need to be performed. Full details on our one hot encoding process can be found under the sub-header, “One Hot Encoding” below.  \n",
    "\n",
    " \n",
    "\n",
    "ASSUMPTIONS OF LOGISTIC REGRESSION MODELS: \n",
    "\n",
    " \n",
    "\n",
    "EXPLORATORY DATA ANALYSIS: \n",
    "\n",
    "\n",
    "***\n",
    "# 3. RESULTS\n",
    "\n",
    "The use of Sklearn's `LogisticRegression` was used to model the data for this case study.\n",
    "\n",
    "*Models use: 10-fold Cross validation (`Kfold`), `random_seed = 0`, and `max_iter = 50000`, and scoring metric of `F1`*\n",
    "\n",
    "**Model HalvingRandomSearchCV:**\n",
    "\n",
    "\n",
    "After preprocessing, EDA, and scaling the data, modeling was able to begin. To determine the best hyperparameters that we should use, we needed to iterate through several of `sklearn's` modules. We began with utilizing `GridSearchCV`, but due to the shape of our data and inability to scale our CPU, GPU, and Memory for the needs of this project, `GridSearchCV` was unable to complete and we needed to try other methods of tuning hyperparameters. With the use of Skelearn's `experimental` and `model_selection` packages we were able to utilize `HalvingRandomSearchCV` to obtain good, but potentially not the best hyperparameters for this model. `HalvingRandomSearchCV` combines the idea of `HalvingSearchCV` and `RandomizedSearchCV`.\n",
    "HalvingSearchCV works by modeliong all potential candidates with less data and selects half of the best performing models to add additional resources and data until a \"best\" model is output. `RandomizedSearchCV` randomly picks candidate modles from the grid to model. \n",
    "\n",
    "We passed the below parameters into `HalvingRandomSearchCV` and the best model outputs were the following:\n",
    "\n",
    "**Halving Random Search CV Parameters:**\n",
    "\n",
    "```\n",
    "        \"C\":            np.logspace(-3,3,7), \n",
    "        \"l1_ratio\":     np.arange(0.0,1.0,0.1),\n",
    "        'solver':       ['saga'],\n",
    "        'penalty':      ['elasticnet'],\n",
    "        \"tol\":          [1e-9,1e-8,1e-7,1e-6,1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "```\n",
    "\n",
    "**Best Model Output:**\n",
    "\n",
    "```\n",
    "        \"C\":            100.0\n",
    "        \"l1_ratio\":     0.8\n",
    "        \"n_jobs\":       -1\n",
    "        \"penalty\":      'elasticnet'\n",
    "        \"solver\":       'saga'\n",
    "        \"tol\":          0.001\n",
    "```\n",
    "\n",
    "**ElasticNetCV with GridSearchCV Tuned Parameters:**\n",
    "\n",
    "After performing `HalvingRandomSearchCV` to tune the model parameters, Sklearn's `cross_validate` was used to validate the model and determine final performance. The results of all 10 folds are below with a mean F1 score of .568746.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th>fit_time</th>\n",
    "        <th>score_time</th>\n",
    "        <th>estimator</th>\n",
    "        <th>test_score</th>\n",
    "        <th>train_score</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>2.43238</td>\n",
    "        <td>0.00473499</td>\n",
    "        <td>LogisticRegression(C=100.0, l1_ratio=0.8, n_jobs=-1, penalty='elasticnet',random_state=0, solver='saga', tol=0.001)</td>\n",
    "        <td>0.568733</td>\n",
    "        <td>0.568562</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>2.60263</td>\n",
    "        <td>0.00420213</td>\n",
    "        <td>LogisticRegression(C=100.0, l1_ratio=0.8, n_jobs=-1, penalty='elasticnet',random_state=0, solver='saga', tol=0.001)</td>\n",
    "        <td>0.56834</td>\n",
    "        <td>0.568627</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td>2.9399</td>\n",
    "        <td>0.00471711</td>\n",
    "        <td>LogisticRegression(C=100.0, l1_ratio=0.8, n_jobs=-1, penalty='elasticnet',random_state=0, solver='saga', tol=0.001)</td>\n",
    "        <td>0.567456</td>\n",
    "        <td>0.568824</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>3</td>\n",
    "        <td>2.86625</td>\n",
    "        <td>0.00472498</td>\n",
    "        <td>LogisticRegression(C=100.0, l1_ratio=0.8, n_jobs=-1, penalty='elasticnet',random_state=0, solver='saga', tol=0.001)</td>\n",
    "        <td>0.566277</td>\n",
    "        <td>0.56902</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>4</td>\n",
    "        <td>2.24447</td>\n",
    "        <td>0.00615811</td>\n",
    "        <td>LogisticRegression(C=100.0, l1_ratio=0.8, n_jobs=-1, penalty='elasticnet',random_state=0, solver='saga', tol=0.001)</td>\n",
    "        <td>0.56726</td>\n",
    "        <td>0.568922</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>5</td>\n",
    "        <td>2.248</td>\n",
    "        <td>0.00481105</td>\n",
    "        <td>LogisticRegression(C=100.0, l1_ratio=0.8, n_jobs=-1, penalty='elasticnet',random_state=0, solver='saga', tol=0.001)</td>\n",
    "        <td>0.572271</td>\n",
    "        <td>0.568409</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>6</td>\n",
    "        <td>2.93157</td>\n",
    "        <td>0.00560784</td>\n",
    "        <td>LogisticRegression(C=100.0, l1_ratio=0.8, n_jobs=-1, penalty='elasticnet',random_state=0, solver='saga', tol=0.001)</td>\n",
    "        <td>0.574194</td>\n",
    "        <td>0.567999</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>7</td>\n",
    "        <td>2.92202</td>\n",
    "        <td>0.00460625</td>\n",
    "        <td>LogisticRegression(C=100.0, l1_ratio=0.8, n_jobs=-1, penalty='elasticnet',random_state=0, solver='saga', tol=0.001)</td>\n",
    "        <td>0.573015</td>\n",
    "        <td>0.568162</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>8</td>\n",
    "        <td>2.47317</td>\n",
    "        <td>0.00449204</td>\n",
    "        <td>LogisticRegression(C=100.0, l1_ratio=0.8, n_jobs=-1, penalty='elasticnet',random_state=0, solver='saga', tol=0.001)</td>\n",
    "        <td>0.561812</td>\n",
    "        <td>0.56956</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>9</td>\n",
    "        <td>2.8959</td>\n",
    "        <td>0.00579691</td>\n",
    "        <td>LogisticRegression(C=100.0, l1_ratio=0.8, n_jobs=-1, penalty='elasticnet',random_state=0, solver='saga', tol=0.001)</td>\n",
    "        <td>0.568101</td>\n",
    "        <td>0.568796</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>MEAN</td>\n",
    "        <td>2.65563</td>\n",
    "        <td>0.00498514</td>\n",
    "        <td></td>\n",
    "        <td>0.568746</td>\n",
    "        <td>0.568688</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "***\n",
    "\n",
    "# 4. CONCLUSION\n",
    "\n",
    "In conclusion,\n",
    "\n",
    " we have determined that logistic regression does not properly model this data due to inablilty for the coefficient's to converge. Potential models that woudl be better for this data set would include decision trees or any sort of gradient boosting. \n",
    "\n",
    "***\n",
    "\n",
    "# 5. CODE:\n",
    "\n",
    "Attached in file CS2_CODE.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "64a98b05f8fc9eecf2bb5338f68e06ef5a87c8de5d8181b9d94599cc0446591d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
