{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 2: Predicting Critical Temperature\n",
    "By: Allen Hoskins and Brittany Lewandowski\n",
    "\n",
    "***\n",
    "# 1. INTRODUCTION\n",
    "\n",
    "\n",
    "Diabetes is a metabolic disease impacting 37.3 million Americans. Those affected by the disease have complications producing insulin, a chemical messenger that our body uses to store energy. Although it is uncommon, diabetics can be hospitalized for having critically low or high blood glucose levels. These hospitalizations can be life threatening and should be minimized at all costs.  \n",
    "\n",
    " \n",
    "\n",
    "In this case study, we will use a diabetes data set procured by Dr. Slater, to identify what factors most significantly result in diabetics getting readmitted to hospitals. To accomplish this, we will build a Logistic Regression model and extract its respective feature importances. It is our hope that this research can be leveraged by medical professionals to help treat hospitalized diabetics and to ensure that these patients are not readmitted in the future.  \n",
    "\n",
    "***\n",
    "\n",
    "# 2. METHODS\n",
    "\n",
    "\n",
    "#### DATA UNDERSTANDING: \n",
    "\n",
    "Data used in this case study was a diabetes.csv provided by Dr. Slater. Our diabetes.csv contained data related to hospitalized diabetic patients including columns such as: “readmitted,” “patient_nbr,” “insulin,” and “time_in_hospital.” Upon reviewing the contents of our data set, we saved the data into a dataframe named “diabetes_data” and began pre-processing. \n",
    "\n",
    "\n",
    "#### DATA PREPROCESSING: \n",
    "\n",
    "\n",
    "The first step we performed in pre-processing was reviewing our full data set. Immediately, we recognized that missing values existed in the columns of:  \n",
    "\n",
    "> race \n",
    "> weight \n",
    "> payer_code \n",
    "> medical_specialty \n",
    "> diag_1 \n",
    "> diag_2 \n",
    "> diag_3 \n",
    "\n",
    "After identifying these missing values, we then ran the command, `diabetes_data.dtypes.value_counts()` and saw that our data set contained 37 categorical columns and 13 numeric columns. Noticing that our data set contained categorical columns, we noted that one hot encoding would need to be performed. Full details on our one hot encoding process can be found under the sub-header, “One Hot Encoding” below.  \n",
    "\n",
    " \n",
    "\n",
    "ASSUMPTIONS OF LOGISTIC REGRESSION MODELS: \n",
    "\n",
    " \n",
    "\n",
    "EXPLORATORY DATA ANALYSIS: \n",
    "\n",
    "\n",
    "***\n",
    "# 3. RESULTS\n",
    "\n",
    "For modeling, we determined that using Sklearn's `ElasticNetCV` model was appropriate as it combines the `l1` and `l2` regularization of `Lasso` and `Ridge` models.\n",
    "\n",
    "*Models use: 10-fold Cross validation (`Kfold`), `random_seed = 0`, and `max_iter = 20000`.*\n",
    "\n",
    "**Model GridSearchCV:**\n",
    "\n",
    "After preprocessing, EDA, and scaling the data, modeling was able to begin. Utilizing the power of Sklearn's `GridSearchCV`, the hyperparameters of `l1_ratio`, `tol`, and `eps` were run though the model and the best output using the `neg_root_mean_squared_error` scoring were output to be used in the final model.\n",
    "\n",
    "**Grid Search Parameters:**\n",
    "\n",
    "```\n",
    "      \"l1_ratio\": np.arange(0.0, 1.0, 0.1), \n",
    "      \"tol\":      [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n",
    "      \"eps\":      [1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
    "```\n",
    "\n",
    "**Best Model Output:**\n",
    "\n",
    "```\n",
    "      \"l1_ratio\": 0.2\n",
    "      \"tol\":      0.01\n",
    "      \"eps\":      0.001\n",
    "```\n",
    "\n",
    "**ElasticNetCV with GridSearchCV Tuned Parameters:**\n",
    "\n",
    "After performing GridSearchCV to tune the model parameters, Sklearn's `cross_validate` was used to validate the model and determine final performance. The results of all 10 folds are below with a mean RMSE of 16.4637.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th>fit_time</th>\n",
    "        <th>score_time</th>\n",
    "        <th>estimator</th>\n",
    "        <th>test_score</th>\n",
    "        <th>train_score</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>2.19916</td>\n",
    "        <td>0.00117993</td>\n",
    "        <td>ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01)</td>\n",
    "        <td>16.7378</td>\n",
    "        <td>16.3605</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>2.15888</td>\n",
    "        <td>0.00172806</td>\n",
    "        <td>ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01)</td>\n",
    "        <td>16.7811</td>\n",
    "        <td>16.3389</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td>2.09171</td>\n",
    "        <td>0.00159836</td>\n",
    "        <td>ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01)</td>\n",
    "        <td>16.2488</td>\n",
    "        <td>16.4508</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>3</td>\n",
    "        <td>2.13909</td>\n",
    "        <td>0.00134301</td>\n",
    "        <td>ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01)</td>\n",
    "        <td>16.4203</td>\n",
    "        <td>16.4222</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>4</td>\n",
    "        <td>2.04526</td>\n",
    "        <td>0.00174427</td>\n",
    "        <td>ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01)</td>\n",
    "        <td>16.5608</td>\n",
    "        <td>16.4041</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>5</td>\n",
    "        <td>2.05884</td>\n",
    "        <td>0.00163698</td>\n",
    "        <td>ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01)</td>\n",
    "        <td>15.7617</td>\n",
    "        <td>16.4887</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>6</td>\n",
    "        <td>2.1159</td>\n",
    "        <td>0.00138688</td>\n",
    "        <td>ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01)</td>\n",
    "        <td>16.2624</td>\n",
    "        <td>16.446</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>7</td>\n",
    "        <td>2.06758</td>\n",
    "        <td>0.00157595</td>\n",
    "        <td>ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01)</td>\n",
    "        <td>16.7527</td>\n",
    "        <td>16.397</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>8</td>\n",
    "        <td>2.10658</td>\n",
    "        <td>0.00192094</td>\n",
    "        <td>ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01)</td>\n",
    "        <td>16.4909</td>\n",
    "        <td>16.4127</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>9</td>\n",
    "        <td>2.11754</td>\n",
    "        <td>0.00175214</td>\n",
    "        <td>ElasticNetCV(l1_ratio=0.2, max_iter=10000, random_state=0, tol=0.01)</td>\n",
    "        <td>16.6208</td>\n",
    "        <td>16.4005</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>MEAN</td>\n",
    "        <td>2.11005</td>\n",
    "        <td>0.00158665</td>\n",
    "        <td></td>\n",
    "        <td>16.4637</td>\n",
    "        <td>16.4121</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**Feature Importance:**\n",
    "\n",
    "After completing the ElasticNetCV modeling, we wanted to determine which features were the most important in predicting `critical_temp`.\n",
    "\n",
    "The top 10 features in the model were:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th>Feature</th>\n",
    "        <th>Coefficient</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>19</td>\n",
    "        <td>Cu</td>\n",
    "        <td>4.99197</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>7</td>\n",
    "        <td>Ba</td>\n",
    "        <td>4.88638</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>12</td>\n",
    "        <td>Ca</td>\n",
    "        <td>4.83552</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>38</td>\n",
    "        <td>La</td>\n",
    "        <td>-3.48937</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>163</td>\n",
    "        <td>wtd_std_Valence</td>\n",
    "        <td>-3.24568</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>9</td>\n",
    "        <td>Bi</td>\n",
    "        <td>2.40671</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>162</td>\n",
    "        <td>wtd_std_ThermalConductivity</td>\n",
    "        <td>2.29054</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>57</td>\n",
    "        <td>Pr</td>\n",
    "        <td>-2.27065</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>31</td>\n",
    "        <td>Hg</td>\n",
    "        <td>2.2417</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>146</td>\n",
    "        <td>wtd_mean_ThermalConductivity</td>\n",
    "        <td>1.89972</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "***\n",
    "\n",
    "# 4. CONCLUSION\n",
    "\n",
    "***\n",
    "\n",
    "# 5. CODE:\n",
    "\n",
    "Attached in file CS2_CODE.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "64a98b05f8fc9eecf2bb5338f68e06ef5a87c8de5d8181b9d94599cc0446591d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
